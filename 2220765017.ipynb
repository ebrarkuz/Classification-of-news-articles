{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## AIN313 ASSIGMENT 2\n",
    "\n",
    "Name: Ebrar Pınar Kuz\n",
    "\n",
    "Student ID: 2220765017"
   ],
   "id": "81e7be99d7d4ecb9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:45.182546Z",
     "start_time": "2025-11-01T17:35:45.164986Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:45.241443Z",
     "start_time": "2025-11-01T17:35:45.200051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(\"English Dataset.csv\")\n",
    "print(df.head())\n",
    "print(df.shape)"
   ],
   "id": "f556ee12e6d02060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ArticleId                                               Text  Category\n",
      "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
      "1        154  german business confidence slides german busin...  business\n",
      "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
      "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
      "4        917  enron bosses in $168m payout eighteen former e...  business\n",
      "(1490, 3)\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PART 1- Understanding the data ",
   "id": "8aa7ab6271a41385"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:45.358112Z",
     "start_time": "2025-11-01T17:35:45.336381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df['Category'].value_counts())\n",
    "df['Text'] = df['Text'].str.lower()\n"
   ],
   "id": "364cc629f06eeb5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category\n",
      "sport            346\n",
      "business         336\n",
      "politics         274\n",
      "entertainment    273\n",
      "tech             261\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:46.113555Z",
     "start_time": "2025-11-01T17:35:45.419178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words= None,\n",
    "    token_pattern=r'(?u)\\b[a-zA-Z]+\\b'  )\n",
    "X = vectorizer.fit_transform(df['Text'])\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "word_freq_df = pd.DataFrame(0, index=vocab, columns=df['Category'].unique())\n",
    "\n",
    "for cat in df['Category'].unique():\n",
    "    subset_index = (df['Category'] == cat).to_numpy()  \n",
    "    counts = X[subset_index]\n",
    "    word_freq = np.sum(counts.toarray(), axis=0)\n",
    "    word_freq_df.loc[:, cat] = word_freq\n",
    "top_words = word_freq_df.sum(axis=1).sort_values(ascending=False).head(30)\n",
    "result_df = word_freq_df.loc[top_words.index]\n",
    "\n",
    "\n",
    "print(result_df)"
   ],
   "id": "cc3e620545c099d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      business  tech  politics  sport  entertainment\n",
      "the       7133  7498      7957   6620           5822\n",
      "to        3306  4149      3913   3189           2108\n",
      "of        2864  3425      2840   1826           2048\n",
      "and       2161  3017      2559   2532           2130\n",
      "a         2296  2773      2532   2651           1898\n",
      "in        2821  2316      2159   2510           1996\n",
      "s         1367   806      1074   1440           1247\n",
      "for       1045  1299      1237   1127           1097\n",
      "is        1072  1597      1167    985            684\n",
      "that      1052  1676      1195    863            491\n",
      "it        1011  1465       998    974            738\n",
      "on         906  1111      1186   1014            881\n",
      "said      1100  1064      1445    636            594\n",
      "was        596   653      1013    943            818\n",
      "he         287   484      1410   1105            584\n",
      "be         573  1057      1080    614            502\n",
      "with       612   787       645    803            662\n",
      "has        835   686       611    650            471\n",
      "as         605   929       690    547            479\n",
      "have       554   731       669    812            384\n",
      "at         609   533       564    794            554\n",
      "by         718   727       651    430            460\n",
      "will       520   797       642    575            433\n",
      "but        391   527       668    992            326\n",
      "are        538   994       635    356            331\n",
      "i           83   205       382   1304            424\n",
      "from       588   500       466    481            356\n",
      "not        302   535       662    490            251\n",
      "they       204   630       572    414            241\n",
      "mr         393   349      1073      8            151\n"
     ]
    }
   ],
   "execution_count": 219
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### REPORT \n",
    "\n",
    "The counts of the categories are very close to each other. The most common category is sport with 346 news articles, and the least common category is tech with 261 news articles.\n",
    "\n",
    "When we look at the counts of common words for each category, we see that:\n",
    "\n",
    "For the business category: said,year, mr, market, new, and firm\n",
    "\n",
    "For the tech category: said,people, mr, new, mobile, technology\n",
    "\n",
    "For the politics category: said,mr, labour, government, election, blair\n",
    "\n",
    "For the sport category: said,game, year, england, time, win\n",
    "\n",
    "For the entertainment category: said,film, best, year, music\n",
    "\n",
    "These are the most common words. I decided on these words just by looking at their counts in each category, but this alone does not give accurate results for making comments. Most of these words appear in more than one category. Therefore, we should choose words that have a high count in one category and a very low count in the others."
   ],
   "id": "1902f19d3e8dcb5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:46.184593Z",
     "start_time": "2025-11-01T17:35:46.153042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Here I check the ratios of the words so, I saw if they make a good representation for classes or not.\n",
    "ratio_df = word_freq_df.div(word_freq_df.sum(axis=1) + 1e-9, axis=0) \n",
    "max_cat = ratio_df.idxmax(axis=1)\n",
    "max_val = ratio_df.max(axis=1)\n",
    "\n",
    "\n",
    "all_representative_words = pd.DataFrame({\n",
    "    'main_category': max_cat,\n",
    "    'strength': max_val,\n",
    "    'total_count': word_freq_df.sum(axis=1) \n",
    "})\n",
    "\n",
    "min_appearances = 5 \n",
    "filtered_rep_words = all_representative_words[\n",
    "    all_representative_words['total_count'] > min_appearances\n",
    "]\n",
    "\n",
    "top_3_per_category_df = filtered_rep_words.groupby('main_category') \\\n",
    "                                          .apply(lambda x: x.sort_values('strength', ascending=False).head(3))\n",
    "\n",
    "top_3_per_category_df = top_3_per_category_df.reset_index(level=0, drop=True)\n",
    "\n",
    "final_word_list = top_3_per_category_df.index\n",
    "report_stats_df = word_freq_df.loc[final_word_list]\n",
    "\n",
    "\n",
    "display(report_stats_df)"
   ],
   "id": "f42f1391fbefeee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          business  tech  politics  sport  entertainment\n",
       "yukos          122     0         0      0              0\n",
       "gm              55     0         0      0              0\n",
       "worldcom        54     0         0      0              0\n",
       "actress          0     0         0      0            116\n",
       "festival         0     0         0      0            100\n",
       "aviator          0     0         0      0             76\n",
       "tory             0     0       170      0              0\n",
       "tories           0     0       158      0              0\n",
       "lib              0     0       133      0              0\n",
       "roddick          0     0         0     97              0\n",
       "referee          0     0         0     66              0\n",
       "athens           0     0         0     64              0\n",
       "spam             0    99         0      0              0\n",
       "mobiles          0    94         0      0              0\n",
       "gadgets          0    88         0      0              0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business</th>\n",
       "      <th>tech</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>entertainment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>yukos</th>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gm</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worldcom</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actress</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>festival</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aviator</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tory</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tories</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lib</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roddick</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>referee</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>athens</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobiles</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gadgets</th>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 220
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### REPORT PART 1: Understanding the Data\n",
    "\n",
    "To determine if predicting a category from its text is feasible, we analyzed the words that are most representative of each category. Instead of using raw word counts, which can be misleading, we analyzed the word frequency *ratios*. This method checks how strongly a word is associated with one specific category compared to all others.\n",
    "\n",
    "Based on this ratio analysis, we can say that specific keywords can successfully represent their main category. The following are three specific keyword examples for each class that we found to be useful:\n",
    "\n",
    "* Business class: `yukos`, `gm`, `worldcom`\n",
    "* Tech class: `spam`, `mobiles`, `gadgets`\n",
    "* Politics class: `tory`, `tories`, `lib`\n",
    "* Sport class: `roddick`, `referee`, `athens`\n",
    "* Entertainment class: `actress`, `festival`, `aviator`"
   ],
   "id": "1f6788f0d6286fa7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 2-  Implementing Naive Bayes",
   "id": "10776100dfb01acb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:46.249620Z",
     "start_time": "2025-11-01T17:35:46.236356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#implementing naive bayes algorithm\n",
    "class MyNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_word_counts = {}\n",
    "        self.class_total_words = {}\n",
    "        self.class_priors = {}\n",
    "\n",
    "        \n",
    "        self.vocab_size = X.shape[1]\n",
    "\n",
    "        for c in self.classes:\n",
    "            X_c = X[np.array(y) == c]\n",
    "\n",
    "            self.class_word_counts[c] = np.sum(X_c, axis=0) + 1  # Laplace smoothing\n",
    "            self.class_total_words[c] = np.sum(self.class_word_counts[c])\n",
    "            self.class_priors[c] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            for c in self.classes:\n",
    "                log_prior = np.log(self.class_priors[c])\n",
    "                log_likelihood = np.sum(x.multiply(np.log(self.class_word_counts[c] / self.class_total_words[c])))\n",
    "                class_probs[c] = log_prior + log_likelihood\n",
    "            preds.append(max(class_probs, key=class_probs.get))\n",
    "        return preds\n"
   ],
   "id": "c1d17f5ba8f5209c",
   "outputs": [],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:46.361065Z",
     "start_time": "2025-11-01T17:35:46.354405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(vectorizer, X_text, y, label):\n",
    "    \n",
    "    if hasattr(vectorizer, \"vocabulary_\"):\n",
    "        X = vectorizer.transform(X_text)\n",
    "    else:\n",
    "        X = vectorizer.fit_transform(X_text)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = MyNaiveBayes()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"{label}: {acc*100:.2f}%\")\n",
    "    return acc\n"
   ],
   "id": "8773391d523c1373",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:47.380783Z",
     "start_time": "2025-11-01T17:35:46.431553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "acc_uni = evaluate_model(\n",
    "    CountVectorizer(stop_words=None, ngram_range=(1,1)),\n",
    "    df[\"Text\"], df[\"Category\"], \"BoW (Unigram) - NB\"\n",
    ")"
   ],
   "id": "71b2318679840a71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Unigram) - NB: 97.65%\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:51.573175Z",
     "start_time": "2025-11-01T17:35:47.395955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "acc_bi = evaluate_model(\n",
    "    CountVectorizer(stop_words=None, ngram_range=(2,2)),\n",
    "    df[\"Text\"], df[\"Category\"], \"BoW (Bigram) - NB\"\n",
    ")"
   ],
   "id": "a1b937b057abe94e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Bigram) - NB: 96.98%\n"
     ]
    }
   ],
   "execution_count": 224
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PART 3-",
   "id": "fa272a45af92a533"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:52.027254Z",
     "start_time": "2025-11-01T17:35:51.616674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer_part3 = CountVectorizer(stop_words=None, ngram_range=(1,1))\n",
    "\n",
    "\n",
    "X_part3 = vectorizer_part3.fit_transform(df[\"Text\"])\n",
    "y_part3 = df[\"Category\"]\n",
    "\n",
    "\n",
    "vocab_part3 = vectorizer_part3.get_feature_names_out()\n",
    "\n",
    "\n",
    "model_part3 = MyNaiveBayes()\n",
    "model_part3.fit(X_part3, y_part3)\n",
    "\n",
    "\n",
    "classes_part3 = model_part3.classes"
   ],
   "id": "47ed289dd9b5d89e",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:52.094685Z",
     "start_time": "2025-11-01T17:35:52.069397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Part 3a: \n",
    "print(\"Top 10 Presence and Absence Words for each class\")\n",
    "\n",
    "likelihoods = {}\n",
    "for c in classes_part3:\n",
    "    \n",
    "    probs = np.asarray(model_part3.class_word_counts[c] / model_part3.class_total_words[c]).flatten()\n",
    "    likelihoods[c] = probs\n",
    "\n",
    "\n",
    "likelihood_df = pd.DataFrame(likelihoods, index=vocab_part3)\n",
    "\n",
    "presence_words = {}\n",
    "absence_words = {}\n",
    "for c in classes_part3:\n",
    "    print(f\"\\nCategory: {c}\")\n",
    "    \n",
    "    presence_words[c] = likelihood_df[c].sort_values(ascending=False).head(10)\n",
    "    print(\"Top 10 presence words:\")\n",
    "    print(presence_words[c])\n",
    "\n",
    "    \n",
    "    absence_words[c] = likelihood_df[c].sort_values(ascending=True).head(10)\n",
    "    print(\"\\nTop 10 absence words:\")\n",
    "    print(absence_words[c])"
   ],
   "id": "1adbf055170e6502",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Presence and Absence Words for each class\n",
      "\n",
      "Category: business\n",
      "Top 10 presence words:\n",
      "the     0.053576\n",
      "to      0.024836\n",
      "of      0.021516\n",
      "in      0.021193\n",
      "and     0.016237\n",
      "said    0.008268\n",
      "is      0.008058\n",
      "that    0.007908\n",
      "for     0.007855\n",
      "it      0.007600\n",
      "Name: business, dtype: float64\n",
      "\n",
      "Top 10 absence words:\n",
      "04secs       0.000008\n",
      "0400         0.000008\n",
      "033          0.000008\n",
      "028          0.000008\n",
      "0130         0.000008\n",
      "0100         0.000008\n",
      "007          0.000008\n",
      "0051         0.000008\n",
      "001st        0.000008\n",
      "mudslides    0.000008\n",
      "Name: business, dtype: float64\n",
      "\n",
      "Category: entertainment\n",
      "Top 10 presence words:\n",
      "the    0.051877\n",
      "and    0.018985\n",
      "to     0.018789\n",
      "of     0.018254\n",
      "in     0.017791\n",
      "for    0.009782\n",
      "on     0.007858\n",
      "was    0.007296\n",
      "it     0.006584\n",
      "is     0.006103\n",
      "Name: entertainment, dtype: float64\n",
      "\n",
      "Top 10 absence words:\n",
      "0530gmt     0.000009\n",
      "01          0.000009\n",
      "028         0.000009\n",
      "03          0.000009\n",
      "zib         0.000009\n",
      "zidane      0.000009\n",
      "ziers       0.000009\n",
      "zinc        0.000009\n",
      "zinedine    0.000009\n",
      "zip         0.000009\n",
      "Name: entertainment, dtype: float64\n",
      "\n",
      "Category: politics\n",
      "Top 10 presence words:\n",
      "the     0.055424\n",
      "to      0.027259\n",
      "of      0.019786\n",
      "and     0.017829\n",
      "in      0.015043\n",
      "said    0.010071\n",
      "he      0.009827\n",
      "for     0.008622\n",
      "that    0.008330\n",
      "on      0.008267\n",
      "Name: politics, dtype: float64\n",
      "\n",
      "Top 10 absence words:\n",
      "zombie      0.000007\n",
      "zola        0.000007\n",
      "zoellick    0.000007\n",
      "zoe         0.000007\n",
      "zodiac      0.000007\n",
      "ziyi        0.000007\n",
      "zip         0.000007\n",
      "zinedine    0.000007\n",
      "04secs      0.000007\n",
      "04bn        0.000007\n",
      "Name: politics, dtype: float64\n",
      "\n",
      "Category: sport\n",
      "Top 10 presence words:\n",
      "the    0.049102\n",
      "to     0.023657\n",
      "and    0.018785\n",
      "in     0.018622\n",
      "of     0.013549\n",
      "for    0.008365\n",
      "he     0.008202\n",
      "on     0.007527\n",
      "but    0.007364\n",
      "is     0.007312\n",
      "Name: sport, dtype: float64\n",
      "\n",
      "Top 10 absence words:\n",
      "0100      0.000007\n",
      "007       0.000007\n",
      "0051      0.000007\n",
      "001st     0.000007\n",
      "001and    0.000007\n",
      "001       0.000007\n",
      "000th     0.000007\n",
      "000bn     0.000007\n",
      "06        0.000007\n",
      "05bn      0.000007\n",
      "Name: sport, dtype: float64\n",
      "\n",
      "Category: tech\n",
      "Top 10 presence words:\n",
      "the     0.049103\n",
      "to      0.027174\n",
      "of      0.022433\n",
      "and     0.019762\n",
      "in      0.015172\n",
      "that    0.010981\n",
      "is      0.010464\n",
      "it      0.009599\n",
      "for     0.008512\n",
      "on      0.007281\n",
      "Name: tech, dtype: float64\n",
      "\n",
      "Top 10 absence words:\n",
      "zooropa    0.000007\n",
      "033        0.000007\n",
      "04         0.000007\n",
      "0400       0.000007\n",
      "041        0.000007\n",
      "04bn       0.000007\n",
      "04secs     0.000007\n",
      "050        0.000007\n",
      "050505     0.000007\n",
      "01         0.000007\n",
      "Name: tech, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 226
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 3a: Analyzing the Effect of Words\n",
    "\n",
    "We looked at the \"Top 10 presence words\" and \"Top 10 absence words\" for each category, based on the probabilities calculated by our model.\n",
    "\n",
    "**1. Presence Words**\n",
    "\n",
    "As the output lists show, the **Top 10 presence words** for every single category are common stopwords. Words like `the`, `to`, `of`, and `and` are at the top of all lists.\n",
    "\n",
    "This happens because our model was trained *with* these words, and they appear very often in all texts. So, the model's math correctly finds that they have the highest probability. However, these words are not informative and do not help us tell the categories apart from each other.\n",
    "\n",
    "**2. Absence Words**\n",
    "\n",
    "On the other hand, the **Top 10 absence words** lists show the words with the lowest possible probability (around `0.000008`). These lists are mostly filled with very rare words, random numbers, or possible typos that the model has almost never seen (like `04secs`, `zombie`, or `zooropa`).\n",
    "\n",
    "Because the model has no information on these words, it gives them the lowest possible score. We can conclude that—just like the stopword-filled \"presence\" lists—these \"absence\" lists are also not very useful for understanding what makes each category unique. This tells us we need to filter out stopwords to find the real keywords, which we will do in Part 3b."
   ],
   "id": "ccf672490f2fc6f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:52.157628Z",
     "start_time": "2025-11-01T17:35:52.130310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TF-IDF \n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(X_part3)\n",
    "\n",
    "# find the tf-idf value for each word \n",
    "tfidf_scores = np.asarray(tfidf_matrix.mean(axis=0)).ravel()\n",
    "\n",
    "tfidf_df = pd.DataFrame({'word': vocab_part3, 'tfidf': tfidf_scores}).sort_values('tfidf', ascending=False)\n",
    "\n",
    "# select teh most important words\n",
    "N_FEATURES = 2000\n",
    "important_words = set(tfidf_df.head(N_FEATURES)['word'])\n",
    "print(f\"Selected the most importanat {len(important_words)} words with TF-IDF \")\n"
   ],
   "id": "a1822e454f7cba7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected the most importanat 2000 words with TF-IDF \n"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:52.700607Z",
     "start_time": "2025-11-01T17:35:52.192759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TF-IDF with stopwords\n",
    "vectorizer_subset = CountVectorizer(\n",
    "    stop_words=None,\n",
    "    ngram_range=(1, 1),\n",
    "    vocabulary=important_words  \n",
    ")\n",
    "acc_tfd = evaluate_model(\n",
    "    vectorizer_subset,\n",
    "    df[\"Text\"],\n",
    "    df[\"Category\"],\n",
    "    f\"BoW (Unigram - Top {N_FEATURES} TF-IDF Words)\"\n",
    ")"
   ],
   "id": "4810b3b12c3a7a2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Unigram - Top 2000 TF-IDF Words): 97.99%\n"
     ]
    }
   ],
   "execution_count": 228
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To find the optimal number of features for our model, I experimented with different vocabulary sizes (N_FEATURES) based on their TF-IDF scores.\n",
    "\n",
    "I observed that the model's accuracy increased as the number of features grew, peaking at `N_FEATURES = 2000`. However, when more than 2000 words were used, the accuracy began to decrease. This suggests that 2000 features is an optimal value, as it provides the best balance between capturing informative signals and including noise or irrelevant details.So I selected `N_FEATURES = 2000` for this experiment."
   ],
   "id": "30093410ed95c25b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:53.275500Z",
     "start_time": "2025-11-01T17:35:52.779554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TF-IDF without stopwords\n",
    "vectorizer_subset = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    vocabulary=important_words  \n",
    ")\n",
    "acc_tfd_stop = evaluate_model(\n",
    "    vectorizer_subset,\n",
    "    df[\"Text\"],\n",
    "    df[\"Category\"],\n",
    "    f\"BoW (Unigram - Top {N_FEATURES} TF-IDF Words)\"\n",
    ")"
   ],
   "id": "886e2ea6695a8b89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Unigram - Top 2000 TF-IDF Words): 97.65%\n"
     ]
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "50d6e6f80f4a86ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:53.322762Z",
     "start_time": "2025-11-01T17:35:53.293381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Part 3b:\n",
    "print(\"Strongest words for each category(except stopwords)\")\n",
    "\n",
    "\n",
    "non_stop_presence = {}\n",
    "for c in classes_part3:\n",
    "   \n",
    "    all_class_words = likelihood_df[c]\n",
    "    \n",
    "    filtered_words = all_class_words.drop(ENGLISH_STOP_WORDS, errors='ignore')\n",
    "    \n",
    "    top_10_non_stopwords = filtered_words.sort_values(ascending=False).head(10)\n",
    "    \n",
    "    non_stop_presence[c] = top_10_non_stopwords \n",
    "    \n",
    "    print(f\"\\nCategory: {c}\")\n",
    "    print(\"Top 10 important non-stopwords:\")\n",
    "    print(top_10_non_stopwords)"
   ],
   "id": "3661a65ea45781bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strongest words for each category(except stopwords)\n",
      "\n",
      "Category: business\n",
      "Top 10 important non-stopwords:\n",
      "said          0.008268\n",
      "year          0.003432\n",
      "mr            0.002959\n",
      "market        0.002140\n",
      "new           0.002058\n",
      "firm          0.001968\n",
      "growth        0.001938\n",
      "company       0.001908\n",
      "economy       0.001757\n",
      "government    0.001622\n",
      "Name: business, dtype: float64\n",
      "\n",
      "Category: entertainment\n",
      "Top 10 important non-stopwords:\n",
      "said      0.005301\n",
      "film      0.005203\n",
      "best      0.003840\n",
      "year      0.002815\n",
      "music     0.002281\n",
      "new       0.002094\n",
      "awards    0.001648\n",
      "uk        0.001532\n",
      "actor     0.001515\n",
      "number    0.001479\n",
      "Name: entertainment, dtype: float64\n",
      "\n",
      "Category: politics\n",
      "Top 10 important non-stopwords:\n",
      "said          0.010071\n",
      "mr            0.007480\n",
      "labour        0.003447\n",
      "government    0.003239\n",
      "election      0.002960\n",
      "blair         0.002758\n",
      "party         0.002626\n",
      "people        0.002598\n",
      "minister      0.001999\n",
      "new           0.001957\n",
      "Name: politics, dtype: float64\n",
      "\n",
      "Category: sport\n",
      "Top 10 important non-stopwords:\n",
      "said       0.004724\n",
      "game       0.002648\n",
      "year       0.002462\n",
      "england    0.002447\n",
      "time       0.002203\n",
      "win        0.002195\n",
      "world      0.002002\n",
      "players    0.001557\n",
      "cup        0.001535\n",
      "team       0.001528\n",
      "Name: sport, dtype: float64\n",
      "\n",
      "Category: tech\n",
      "Top 10 important non-stopwords:\n",
      "said          0.006974\n",
      "people        0.004243\n",
      "mr            0.002292\n",
      "new           0.002292\n",
      "mobile        0.002252\n",
      "technology    0.001991\n",
      "users         0.001761\n",
      "software      0.001742\n",
      "use           0.001709\n",
      "net           0.001683\n",
      "Name: tech, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 230
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 3b: Analysis of Non-Stopword Lists\n",
    "\n",
    "These lists, which were generated after filtering the stopwords are more informative than the \"presence\" lists from Part 3a, which were dominated by some stopwords words like `the` and `to`.\n",
    "\n",
    "By removing the stopwords, we can now see the **true, informative keywords** that the model is using to identify each category.\n",
    "\n",
    "The word `said` dominates the top of almost every category (it is number 1 in all of them). While `said` is not on the sklearn stopword list, it clearly acts as a stopword for this news dataset. \n",
    "\n",
    "When we pass the common words like 'said' , 'mr' we saw that:\n",
    "    * Politics is clearly identified by `labour`, `government`, `election`, and `blair`.\n",
    "    * Entertainment is identified by `film`, `music`, `awards`, and `actor`.\n",
    "    * Sport is identified by `game`, `england`, `win`, `players`, `cup`, and `team`.\n",
    "    * Tech is identified by `mobile`, `technology`, `users`, `software`, and `net`.\n",
    "\n",
    " \n",
    "\n"
   ],
   "id": "6ea76ba3ff7d4ec9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:54.312755Z",
     "start_time": "2025-11-01T17:35:53.359203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "acc_uni_sw = evaluate_model(\n",
    "    CountVectorizer(stop_words='english', ngram_range=(1,1)),\n",
    "    df[\"Text\"], df[\"Category\"], \"BoW (Unigram) - Stopwords - NB\"\n",
    ")"
   ],
   "id": "e61600e37f73fbd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Unigram) - Stopwords - NB: 97.65%\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:57.850287Z",
     "start_time": "2025-11-01T17:35:54.359209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === 4️⃣ BoW (Bigram + Stopwords Removed) ===\n",
    "acc_bi_sw = evaluate_model(\n",
    "    CountVectorizer(stop_words='english', ngram_range=(2,2)),\n",
    "    df[\"Text\"], df[\"Category\"], \"BoW (Bigram) - Stopwords - NB\"\n",
    ")\n"
   ],
   "id": "a0e645f630bfa9cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW (Bigram) - Stopwords - NB: 94.63%\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T17:35:57.913284Z",
     "start_time": "2025-11-01T17:35:57.895127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Accuracy Summary ===\")\n",
    "print(f\"BoW Unigram: {acc_uni*100:.2f}%\")\n",
    "print(f\"BoW Bigram: {acc_bi*100:.2f}%\")\n",
    "print(f\"BoW Unigram Stopwords: {acc_uni_sw*100:.2f}%\")\n",
    "print(f\"BoW Bigram Stopwords: {acc_bi_sw*100:.2f}%\")\n",
    "print(f\"TF-IDF: {acc_tfd*100:.2f}%\")\n",
    "print(f\"TF-IDF Stopwords: {acc_tfd_stop*100:.2f}%\")\n"
   ],
   "id": "e82deda4474e7879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Accuracy Summary ===\n",
      "BoW Unigram: 97.65%\n",
      "BoW Bigram: 96.98%\n",
      "BoW Unigram Stopwords: 97.65%\n",
      "BoW Bigram Stopwords: 94.63%\n",
      "TF-IDF: 97.99%\n",
      "TF-IDF Stopwords: 97.65%\n"
     ]
    }
   ],
   "execution_count": 233
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### REPORT-About stopwords and Accuracy Results\n",
    "\n",
    "\"Why we remove stop words?\"\n",
    "\n",
    "* As seen in Part 3a, keeping stopwords hides the true signals. The `presence` lists were 100% noise. Removing them acts as a filter, allowing model  to focus on the words that actually carry meaning.\n",
    "* Filtering stopwords allows us to interpret the model. We can now confirm that our model correctly learned that `film` predicts 'Entertainment' and `game` predicts 'Sport'.\n",
    "* Removing stopwords seems so sensible at first but when we look at the results we saw that removing them is getting lower accuracy\n",
    "\n",
    " \"Why we should keep stop words?\"\n",
    "\n",
    "* Removing  stopwords can break some meaningful word pairs. For example, in a Bigram model, the phrase \"King *of* Spain\" is meaningful, and removing \"of\" would break it. So,in the case of BoW bigram the accuracy 96.98 but when we remove stopwords it is 94.63.\n",
    "\n",
    "However, for a Unigram (single word) Naive Bayes classifier, stopwords primarily add noise. We can expect the model's accuracy to be higher *after* removing them but in this case the accuracy did not change. Probable reason is the count of stopwords. It is nor enough to change accuracy.\n",
    "\n",
    "The most interesting result came from the **TF-IDF models**.\n",
    "* The **best accuracy** of all experiments was **97.99%**, which came from the TF-IDF model that kept its stopwords.\n",
    "* When we removed the stopwords first (Model 6), the accuracy actually dropped to *97.65%*.\n",
    "\n",
    "The reason is: TF-IDF is a smarter method. It automatically handles stopwords.  The \"IDF\" part of the algorithm sees that words like 'the' or 'to' are in all documents, so it gives them a very low \"importance\" score.\n",
    "\n",
    "The model (Model 5) already learned to ignore these stopwords on its own. Manually removing them (Model 6) did not help and, just like in the Bigram model, it resulted in a slightly lower accuracy."
   ],
   "id": "82a09aa4eda38c5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
